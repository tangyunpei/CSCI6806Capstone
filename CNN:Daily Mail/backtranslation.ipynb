{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLTXy30MXz2V"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import wordnet\n",
        "import nlpaug.augmenter.word as naw\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "cnn_dataset = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\", split='train')\n",
        "newsqa_dataset = load_dataset(\"glnmario/news-qa-summarization\", split='train')\n",
        "\n",
        "def get_synonym(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if synsets:\n",
        "        synset = synsets[0]\n",
        "        synonyms = synset.lemma_names()\n",
        "        filtered_synonyms = [syn for syn in synonyms if syn.lower() != word.lower()]\n",
        "        if filtered_synonyms:\n",
        "            return filtered_synonyms[0]\n",
        "    return word\n",
        "\n",
        "def augment_with_wordnet(text):\n",
        "    return \" \".join([get_synonym(word) for word in text.split()])\n",
        "\n",
        "nlpaug_augmenter = naw.SynonymAug(aug_p=0.1)\n",
        "\n",
        "def back_translate(text, src_lang='en', tgt_lang='es'):\n",
        "    translator = Translator()\n",
        "    translated = translator.translate(text, src=src_lang, dest=tgt_lang).text\n",
        "    back_translated = translator.translate(translated, src=tgt_lang, dest=src_lang).text\n",
        "    return back_translated\n",
        "\n",
        "def augment_text(text):\n",
        "    back_translated_text = back_translate(text)\n",
        "    wordnet_augmented_text = augment_with_wordnet(back_translated_text)\n",
        "    nlpaug_augmented_text = nlpaug_augmenter.augment(wordnet_augmented_text)\n",
        "    return back_translated_text, wordnet_augmented_text, nlpaug_augmented_text\n",
        "\n",
        "augmented_data_back_translation = []\n",
        "augmented_data_wordnet = []\n",
        "augmented_data_nlpaug = []\n",
        "\n",
        "for i in tqdm(range(len(cnn_dataset))):\n",
        "    example_cnn = cnn_dataset[i]['article']\n",
        "    cnn_back_translated, cnn_wordnet_augmented, cnn_nlpaug_augmented = augment_text(example_cnn)\n",
        "\n",
        "    example_newsqa = newsqa_dataset[i]['story']\n",
        "    newsqa_back_translated, newsqa_wordnet_augmented, newsqa_nlpaug_augmented = augment_text(example_newsqa)\n",
        "\n",
        "    augmented_data_back_translation.append({\n",
        "        'cnn_original': example_cnn,\n",
        "        'cnn_back_translated': cnn_back_translated,\n",
        "        'newsqa_original': example_newsqa,\n",
        "        'newsqa_back_translated': newsqa_back_translated\n",
        "    })\n",
        "\n",
        "    augmented_data_wordnet.append({\n",
        "        'cnn_back_translated': cnn_back_translated,\n",
        "        'cnn_wordnet_augmented': cnn_wordnet_augmented,\n",
        "        'newsqa_back_translated': newsqa_back_translated,\n",
        "        'newsqa_wordnet_augmented': newsqa_wordnet_augmented\n",
        "    })\n",
        "\n",
        "    augmented_data_nlpaug.append({\n",
        "        'cnn_wordnet_augmented': cnn_wordnet_augmented,\n",
        "        'cnn_nlpaug_augmented': cnn_nlpaug_augmented,\n",
        "        'newsqa_wordnet_augmented': newsqa_wordnet_augmented,\n",
        "        'newsqa_nlpaug_augmented': newsqa_nlpaug_augmented\n",
        "    })\n",
        "\n",
        "df_back_translation = pd.DataFrame(augmented_data_back_translation)\n",
        "df_wordnet = pd.DataFrame(augmented_data_wordnet)\n",
        "df_nlpaug = pd.DataFrame(augmented_data_nlpaug)\n",
        "\n",
        "df_back_translation.to_csv(\"/Users/fanyuanhao/python/path/to/save/augmented_dataset_fixed/cnn_newsqa_back_translation.csv\", index=False)\n",
        "df_wordnet.to_csv(\"/Users/fanyuanhao/python/path/to/save/augmented_dataset_fixed/cnn_newsqa_wordnet_augmented.csv\", index=False)\n",
        "df_nlpaug.to_csv(\"/Users/fanyuanhao/python/path/to/save/augmented_dataset_fixed/cnn_newsqa_nlpaug_augmented.csv\", index=False)\n",
        "\n",
        "print(\"All augmented data saved successfully.\")\n"
      ]
    }
  ]
}